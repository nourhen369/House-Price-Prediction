{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from http.client import RemoteDisconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_path = [\n",
    "    \"Immobilier\", \n",
    "    \"ImmoNeuf/Immobilier%20Neuf\"\n",
    "]\n",
    "base_url = \"https://www.tayara.tn/ads/c/{}/?page={}\"\n",
    "\n",
    "def get_all_soup():\n",
    "    soups = []\n",
    "    for category in categories_path:\n",
    "        for page in range(1, 150):  # Limiter à 5 pages pour l'exemple\n",
    "            url = base_url.format(category, page)\n",
    "            # print(url)\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            soups.append(soup)\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permet de retourner tous les liens de chaque immobilier\n",
    "# vs pouvez inspecter https://www.tayara.tn/item/66eae8a467b755ba922a2a58/Immobilier%20Neuf/Ariana/Ghazela/Appartement_en_S2_de_12880_m_A41_au_4me_tage/\n",
    "def get_all_links(soup):\n",
    "    property_links = []\n",
    "    for article in soup.find_all('article', class_=\"mx-0\"):\n",
    "        link = article.find('a')['href']\n",
    "        if '/item/' in link:\n",
    "            full_url = \"https://www.tayara.tn\" + link\n",
    "            property_links.append(full_url)\n",
    "    return property_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = {} # contient toutes les caractéristiques possibles des biens immobiliers\n",
    "\n",
    "def crawl_property_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    item_info = {}\n",
    "\n",
    "    try:\n",
    "        # Titre\n",
    "        title_element = soup.find('h1', class_='text-gray-700 font-bold text-2xl font-arabic')\n",
    "        if not title_element:\n",
    "            # annonce gnrlmnt supprimée, ntaffiwha\n",
    "            print(f\"Titre non disponible pour l'URL: {url}, annonce ignorée.\")\n",
    "            return None\n",
    "        title = title_element.text.strip()\n",
    "\n",
    "        # Configuration des pièces\n",
    "        pieces = re.search(r'S\\+\\d+', title)\n",
    "        pieces_value = pieces.group() if pieces else None\n",
    "\n",
    "        # Récupérer les parties du prix\n",
    "        price_parts = soup.find_all('span', class_='mr-1')\n",
    "        price = ''.join([part.text.strip() for part in price_parts if part.text.strip().isdigit()])\n",
    "\n",
    "        # Localisation\n",
    "        location_element = soup.find('div', class_='flex items-center space-x-2 mb-1')\n",
    "        if location_element:\n",
    "            location = location_element.text.strip().split(',')[0].strip()\n",
    "        else:\n",
    "            location = None\n",
    "\n",
    "        # Ajouter les informations de base\n",
    "        item_info['Annonce'] = title\n",
    "        item_info['Prix'] = price\n",
    "        item_info['Localisation'] = location\n",
    "        item_info['Configuration des pièces'] = pieces_value\n",
    "\n",
    "        # Ajouter dynamiquement les nouvelles clés à item_info\n",
    "        criterias = soup.find_all('span', class_='text-gray-600/80 text-2xs md:text-xs lg:text-xs font-medium')\n",
    "        values = soup.find_all('span', class_='text-gray-700/80 text-xs md:text-sm lg:text-sm font-semibold')\n",
    "\n",
    "        # Associer chaque critère à sa valeur\n",
    "        for criteria, value in zip(criterias, values):\n",
    "            criterion_text = criteria.text.strip()\n",
    "            value_text = value.text.strip()\n",
    "\n",
    "            if criterion_text:\n",
    "                # Ajouter la nouvelle clé à item_info\n",
    "                item_info[criterion_text] = value_text\n",
    "                # Ajouter cette clé dans global_features si elle n'existe pas encore\n",
    "                if criterion_text not in global_features:\n",
    "                    global_features[criterion_text] = 'False'\n",
    "\n",
    "        # Vérifier si toutes les clés de global_features sont présentes dans item_info\n",
    "        for feature in global_features:\n",
    "            if feature not in item_info:\n",
    "                item_info[feature] = 'False'\n",
    "            \n",
    "\n",
    "        return item_info\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Erreur d'extraction à l'URL: {url} - {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_links = []\n",
    "for soup in get_all_soup():\n",
    "    property_link = get_all_links(soup)\n",
    "    property_links.extend(property_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename=\"immobiliers.csv\"):\n",
    "    if not data:\n",
    "        print(\"Aucune donnée à sauvegarder.\")\n",
    "        return\n",
    "\n",
    "    # Créer un ensemble pour les fieldnames\n",
    "    fieldnames = set()\n",
    "\n",
    "    # Passer par toutes les annonces et ajouter dynamiquement les champs\n",
    "    for row in data:\n",
    "        if row:  # Vérifier que la ligne n'est pas vide\n",
    "            fieldnames.update(row.keys())  # Ajouter toutes les clés des données à fieldnames\n",
    "\n",
    "    # Convertir l'ensemble en une liste pour avoir une structure ordonnée\n",
    "    fieldnames = list(fieldnames)\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                if row:  # Vérifier que la ligne n'est pas vide\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Erreur: permission refusée pour écrire dans le fichier {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'enregistrement des données : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for link in property_links:\n",
    "    try:\n",
    "        data.append(crawl_property_page(link))\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Connection error: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Other error: {e}\")\n",
    "    except RemoteDisconnected as e:\n",
    "        print(f\"Remote disconnected: {e}\")\n",
    "save_to_csv(data, \"immobiliers_2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
